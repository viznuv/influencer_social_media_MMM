{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oop_9HtfwPyj",
        "outputId": "fb5db55b-7c0f-4314-abe3-e4f3d48de54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating MMM data...\n",
            "Processing data for modeling...\n",
            "Training model...\n",
            "Model Training Results:\n",
            "Training R²: 0.6847\n",
            "Test R²: 0.2708\n",
            "Training RMSE: 673437.97\n",
            "Test RMSE: 1232405.80\n",
            "Calculating ROI...\n",
            "TV ROI: 25063439.92\n",
            "Digital ROI: 25279680.66\n",
            "Radio ROI: 46769705.70\n",
            "Print ROI: 0.00\n",
            "Social_Media ROI: 60104995.34\n",
            "Influencer ROI: 73617303.93\n",
            "Decomposing sales...\n",
            "Simulating budget allocations...\n",
            "Top budget allocation strategy:\n",
            "TV: 10.8%\n",
            "Digital: 28.9%\n",
            "Radio: 11.8%\n",
            "Print: 4.3%\n",
            "Social_Media: 35.5%\n",
            "Influencer: 8.7%\n",
            "Predicted Sales: 435945722.21\n",
            "Creating visualizations...\n",
            "Visualization complete. Figures available in variables:\n",
            "- spend_fig: Media spend patterns\n",
            "- roi_fig: ROI comparison\n",
            "- response_curves_fig: Channel response curves\n",
            "- decomp_fig: Sales decomposition\n",
            "- budget_fig: Budget optimization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5a517eb5-0e1e-4c06-99e9-7f053e021edd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5a517eb5-0e1e-4c06-99e9-7f053e021edd\")) {                    Plotly.newPlot(                        \"5a517eb5-0e1e-4c06-99e9-7f053e021edd\",                        [{\"marker\":{\"color\":[\"#1a9850\",\"#1a9850\",\"#1a9850\",\"#fdae61\",\"#1a9850\",\"#1a9850\"]},\"text\":[\"25063439.92\",\"25279680.66\",\"46769705.70\",\"0.00\",\"60104995.34\",\"73617303.93\"],\"textposition\":\"outside\",\"x\":[\"TV\",\"Digital\",\"Radio\",\"Print\",\"Social_Media\",\"Influencer\"],\"y\":[25063439.916472,25279680.659563474,46769705.69965871,0,60104995.34084464,73617303.93150055],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"red\",\"dash\":\"dash\",\"width\":2},\"type\":\"line\",\"x0\":-0.5,\"x1\":5.5,\"y0\":1.0,\"y1\":1.0}],\"annotations\":[{\"font\":{\"color\":\"red\"},\"showarrow\":false,\"text\":\"Break-even (ROI=1.0)\",\"x\":\"Influencer\",\"y\":1.0,\"yshift\":5}],\"yaxis\":{\"title\":{\"text\":\"ROI (Return on Ad Spend)\"},\"range\":[0,80979034.32465062]},\"title\":{\"text\":\"Channel ROI Comparison\"},\"xaxis\":{\"title\":{\"text\":\"Channel\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5a517eb5-0e1e-4c06-99e9-7f053e021edd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== MARKETING MIX MODEL SUMMARY REPORT =====\n",
            "\n",
            "MODEL PERFORMANCE\n",
            "Training R²: 0.6847\n",
            "Test R²: 0.2708\n",
            "\n",
            "CHANNEL ROI\n",
            "Influencer: 73617303.93\n",
            "Social_Media: 60104995.34\n",
            "Radio: 46769705.70\n",
            "Digital: 25279680.66\n",
            "TV: 25063439.92\n",
            "Print: 0.00\n",
            "\n",
            "BUDGET OPTIMIZATION\n",
            "Current allocation vs. Optimized allocation:\n",
            "Digital: 20.7% -> 28.9% (↑)\n",
            "Influencer: 5.4% -> 8.7% (↑)\n",
            "Print: 5.8% -> 4.3% (↓)\n",
            "Radio: 11.8% -> 11.8% (↓)\n",
            "Social_Media: 18.1% -> 35.5% (↑)\n",
            "TV: 38.2% -> 10.8% (↓)\n",
            "\n",
            "KEY INSIGHTS:\n",
            "1. Influencer has the highest ROI at 73617303.93\n",
            "2. Digital channels (Digital: 25279680.66, Social: 60104995.34, Influencer: 73617303.93) vs. TV: 25063439.92\n",
            "3. Recommended budget increases: Digital (+8.2%), Social_Media (+17.3%)\n",
            "4. Recommended budget decreases: TV (-27.4%)\n",
            "\n",
            "===== END OF REPORT =====\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# After running your analysis\\nresults = run_mmm_analysis(periods=104, train_model=True, visualize=True)\\n\\n# Save all visualizations\\nsave_mmm_visualizations(results)\\n\\n# Alternatively, create and save visualizations manually\\ncreate_and_save_visualizations(\\n    results['data'], \\n    results['model_results'], \\n    results['roi_results'], \\n    results['simulation_results'],\\n    results.get('contrib_dict')\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ----- EXTENDED DATA GENERATION FUNCTIONS WITH SOCIAL & INFLUENCER CHANNELS -----\n",
        "\n",
        "def generate_date_range(start_date='2022-01-01', periods=104):\n",
        "    \"\"\"Generate a series of dates for weekly data\"\"\"\n",
        "    dates = pd.date_range(start=start_date, periods=periods, freq='W')\n",
        "    return dates\n",
        "\n",
        "def generate_baseline_sales(periods=104, baseline=100000, noise_level=0.05):\n",
        "    \"\"\"Generate baseline sales with some random noise\"\"\"\n",
        "    noise = np.random.normal(0, noise_level, periods)\n",
        "    baseline_trend = np.linspace(0, 0.3, periods)  # Slight upward trend\n",
        "    seasonality = 0.2 * np.sin(np.linspace(0, 8*np.pi, periods))  # Seasonal pattern\n",
        "\n",
        "    sales = baseline * (1 + noise + baseline_trend + seasonality)\n",
        "    return sales\n",
        "\n",
        "def generate_tv_spend(periods=104, budget=40000, noise_level=0.3):\n",
        "    \"\"\"Generate TV advertising spend with budget fluctuations\"\"\"\n",
        "    base_spend = np.ones(periods) * budget\n",
        "    # Add some campaign spikes\n",
        "    campaign_periods = [13, 26, 52, 65, 78, 91]  # Campaigns every quarter\n",
        "    for period in campaign_periods:\n",
        "        base_spend[period-2:period+2] *= 2  # Double spend during campaigns\n",
        "\n",
        "    # Add noise\n",
        "    noise = np.random.normal(0, noise_level, periods)\n",
        "    spend = base_spend * (1 + noise)\n",
        "\n",
        "    # Ensure no negative spend\n",
        "    spend = np.maximum(spend, 0)\n",
        "    return spend\n",
        "\n",
        "def generate_digital_spend(periods=104, budget=25000, noise_level=0.2):\n",
        "    \"\"\"Generate digital advertising spend\"\"\"\n",
        "    base_spend = np.ones(periods) * budget\n",
        "    # Digital tends to be more consistent but with occasional tests\n",
        "    test_periods = [8, 22, 36, 50, 64, 78, 92]\n",
        "    for period in test_periods:\n",
        "        base_spend[period:period+2] *= 1.5  # 50% increase during test periods\n",
        "\n",
        "    # Add noise\n",
        "    noise = np.random.normal(0, noise_level, periods)\n",
        "    spend = base_spend * (1 + noise)\n",
        "\n",
        "    # Ensure no negative spend\n",
        "    spend = np.maximum(spend, 0)\n",
        "    return spend\n",
        "\n",
        "def generate_radio_spend(periods=104, budget=15000, noise_level=0.4):\n",
        "    \"\"\"Generate radio advertising spend\"\"\"\n",
        "    # Radio might be more seasonal\n",
        "    seasonality = 0.3 * np.sin(np.linspace(0, 4*np.pi, periods))\n",
        "    base_spend = budget * (1 + seasonality)\n",
        "\n",
        "    # Add noise\n",
        "    noise = np.random.normal(0, noise_level, periods)\n",
        "    spend = base_spend * (1 + noise)\n",
        "\n",
        "    # Ensure no negative spend\n",
        "    spend = np.maximum(spend, 0)\n",
        "    return spend\n",
        "\n",
        "def generate_print_spend(periods=104, budget=10000, noise_level=0.5):\n",
        "    \"\"\"Generate print advertising spend\"\"\"\n",
        "    # Print might be more sporadic\n",
        "    base_spend = np.random.gamma(shape=1.5, scale=budget/1.5, size=periods)\n",
        "\n",
        "    # Add some zero spend periods (no print ads)\n",
        "    zero_indices = np.random.choice(periods, size=int(periods*0.2), replace=False)\n",
        "    base_spend[zero_indices] = 0\n",
        "\n",
        "    return base_spend\n",
        "\n",
        "# ----- NEW SOCIAL MEDIA & INFLUENCER CHANNEL FUNCTIONS -----\n",
        "\n",
        "def generate_Social_Media_Spend(periods=104, budget=20000, noise_level=0.25):\n",
        "    \"\"\"Generate social media advertising spend with realistic patterns\"\"\"\n",
        "    np.random.seed(50)  # Set a different seed for social media\n",
        "\n",
        "    # Base spend with gradual increase over time (reflecting growing importance)\n",
        "    growth_factor = np.linspace(0.8, 1.2, periods)\n",
        "    base_spend = budget * growth_factor\n",
        "\n",
        "    # Add campaign spikes (more frequent than TV)\n",
        "    campaign_periods = [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96]  # Monthly campaigns\n",
        "    for period in campaign_periods:\n",
        "        if period < periods:\n",
        "            window = 2  # Shorter campaign windows\n",
        "            start_idx = max(0, period-window//2)\n",
        "            end_idx = min(periods, period+window//2)\n",
        "            base_spend[start_idx:end_idx] *= 1.5  # 50% increase during campaigns\n",
        "\n",
        "    # Add noise (more variable than traditional channels)\n",
        "    noise = np.random.normal(0, noise_level, periods)\n",
        "    spend = base_spend * (1 + noise)\n",
        "\n",
        "    # Add occasional viral content pushes\n",
        "    viral_periods = np.random.choice(periods, size=4, replace=False)\n",
        "    for period in viral_periods:\n",
        "        spend[period] *= 2.5  # Higher spend for viral content\n",
        "\n",
        "    # Ensure no negative spend\n",
        "    spend = np.maximum(spend, 0)\n",
        "    return spend\n",
        "\n",
        "def generate_influencer_spend(periods=104, budget=15000, noise_level=0.6):\n",
        "    \"\"\"Generate influencer marketing spend with distinct patterns\"\"\"\n",
        "    np.random.seed(51)  # Set a different seed for influencer\n",
        "\n",
        "    # Influencer marketing tends to be more sporadic with discrete campaigns\n",
        "    base_spend = np.zeros(periods)\n",
        "\n",
        "    # Add campaign periods (larger, less frequent than social)\n",
        "    campaign_periods = [13, 26, 39, 52, 65, 78, 91]  # Quarterly big pushes\n",
        "    for period in campaign_periods:\n",
        "        if period < periods:\n",
        "            # Campaign duration\n",
        "            duration = np.random.choice([2, 3, 4])  # Variable campaign length\n",
        "            for i in range(duration):\n",
        "                if period+i < periods:\n",
        "                    base_spend[period+i] = budget * (1.5 - 0.3*i)  # Decaying impact\n",
        "\n",
        "    # Add some always-on influencer relationships\n",
        "    always_on = budget * 0.2 * np.ones(periods)  # 20% of budget for ongoing relationships\n",
        "    base_spend += always_on\n",
        "\n",
        "    # Add randomness to represent different influencer costs and availability\n",
        "    noise_multiplier = np.random.normal(1, noise_level, periods)\n",
        "    spend = base_spend * noise_multiplier\n",
        "\n",
        "    # Add occasional mega-influencer campaigns (much higher cost)\n",
        "    mega_periods = np.random.choice(periods, size=2, replace=False)\n",
        "    for period in mega_periods:\n",
        "        spend[period] = budget * 3  # 3x normal budget for mega influencers\n",
        "\n",
        "    # Ensure no negative spend\n",
        "    spend = np.maximum(spend, 0)\n",
        "    return spend\n",
        "\n",
        "# ----- TRADITIONAL CHANNEL EFFECT CALCULATION FUNCTIONS -----\n",
        "\n",
        "def calculate_tv_effect(spend, adstock_rate=0.7, saturation=0.7, base_effectiveness=1.5):\n",
        "    \"\"\"Calculate the effect of TV advertising with adstock and saturation\"\"\"\n",
        "    # Apply adstock (lagged effect)\n",
        "    effect = np.zeros(len(spend))\n",
        "    effect[0] = spend[0]\n",
        "    for i in range(1, len(spend)):\n",
        "        effect[i] = spend[i] + adstock_rate * effect[i-1]\n",
        "\n",
        "    # Apply diminishing returns (saturation)\n",
        "    effect = base_effectiveness * np.power(effect, saturation)\n",
        "\n",
        "    return effect\n",
        "\n",
        "def calculate_digital_effect(spend, adstock_rate=0.3, saturation=0.8, base_effectiveness=2.0):\n",
        "    \"\"\"Calculate the effect of digital advertising\"\"\"\n",
        "    # Digital typically has less carryover but higher immediate impact\n",
        "    effect = np.zeros(len(spend))\n",
        "    effect[0] = spend[0]\n",
        "    for i in range(1, len(spend)):\n",
        "        effect[i] = spend[i] + adstock_rate * effect[i-1]\n",
        "\n",
        "    # Apply diminishing returns\n",
        "    effect = base_effectiveness * np.power(effect, saturation)\n",
        "\n",
        "    return effect\n",
        "\n",
        "def calculate_radio_effect(spend, adstock_rate=0.5, saturation=0.6, base_effectiveness=1.2):\n",
        "    \"\"\"Calculate the effect of radio advertising\"\"\"\n",
        "    effect = np.zeros(len(spend))\n",
        "    effect[0] = spend[0]\n",
        "    for i in range(1, len(spend)):\n",
        "        effect[i] = spend[i] + adstock_rate * effect[i-1]\n",
        "\n",
        "    # Apply diminishing returns\n",
        "    effect = base_effectiveness * np.power(effect, saturation)\n",
        "\n",
        "    return effect\n",
        "\n",
        "def calculate_print_effect(spend, adstock_rate=0.4, saturation=0.5, base_effectiveness=1.0):\n",
        "    \"\"\"Calculate the effect of print advertising\"\"\"\n",
        "    effect = np.zeros(len(spend))\n",
        "    effect[0] = spend[0]\n",
        "    for i in range(1, len(spend)):\n",
        "        effect[i] = spend[i] + adstock_rate * effect[i-1]\n",
        "\n",
        "    # Apply diminishing returns\n",
        "    effect = base_effectiveness * np.power(effect, saturation)\n",
        "\n",
        "    return effect\n",
        "\n",
        "# ----- NEW SOCIAL MEDIA & INFLUENCER EFFECT CALCULATION FUNCTIONS -----\n",
        "\n",
        "def calculate_social_media_effect(spend, adstock_rate=0.2, saturation=0.75, base_effectiveness=2.5,\n",
        "                                 viral_coefficient=0.1, viral_decay=0.5):\n",
        "    \"\"\"\n",
        "    Calculate the effect of social media with:\n",
        "    1. Lower adstock (shorter memory) but higher base effectiveness\n",
        "    2. Viral potential (content can organically spread)\n",
        "    3. Network effects\n",
        "    \"\"\"\n",
        "    # Initialize effect array\n",
        "    effect = np.zeros(len(spend))\n",
        "\n",
        "    # Initialize viral component array (to track organic spread)\n",
        "    viral = np.zeros(len(spend))\n",
        "\n",
        "    # Calculate first period normally\n",
        "    effect[0] = spend[0]\n",
        "    viral[0] = 0\n",
        "\n",
        "    # Calculate remaining periods with viral effects\n",
        "    for i in range(1, len(spend)):\n",
        "        # Calculate viral component based on previous effect\n",
        "        new_viral = viral_coefficient * effect[i-1]\n",
        "        viral[i] = new_viral + viral_decay * viral[i-1]  # Viral effect decays over time\n",
        "\n",
        "        # Basic adstock effect (shorter memory than traditional channels)\n",
        "        adstock_effect = spend[i] + adstock_rate * effect[i-1]\n",
        "\n",
        "        # Combine paid and viral effect\n",
        "        effect[i] = adstock_effect + viral[i]\n",
        "\n",
        "    # Network effect - social media impact increases when spend is consistently high\n",
        "    # Higher spend creates more significant network effects\n",
        "    rolling_spend = pd.Series(spend).rolling(window=4, min_periods=1).mean().values\n",
        "    network_multiplier = 1 + (0.2 * (rolling_spend - rolling_spend.min()) /\n",
        "                            (rolling_spend.max() - rolling_spend.min() + 1e-10))\n",
        "\n",
        "    # Apply network multiplier\n",
        "    effect = effect * network_multiplier\n",
        "\n",
        "    # Apply diminishing returns (saturation)\n",
        "    effect = base_effectiveness * np.power(effect, saturation)\n",
        "\n",
        "    return effect\n",
        "\n",
        "def calculate_influencer_effect(spend, adstock_rate=0.4, saturation=0.6, base_effectiveness=2.0,\n",
        "                               authenticity_factor=0.2, max_authenticity_bonus=0.5):\n",
        "    \"\"\"\n",
        "    Calculate the effect of influencer marketing with:\n",
        "    1. Medium adstock (medium memory)\n",
        "    2. Non-linear authenticity effects (less spend can sometimes be more effective)\n",
        "    3. Diminishing returns as influencer saturation increases\n",
        "    \"\"\"\n",
        "    # Initialize effect array\n",
        "    effect = np.zeros(len(spend))\n",
        "    effect[0] = spend[0]\n",
        "\n",
        "    # Basic adstock calculation\n",
        "    for i in range(1, len(spend)):\n",
        "        effect[i] = spend[i] + adstock_rate * effect[i-1]\n",
        "\n",
        "    # Calculate authenticity bonus\n",
        "    # Influencer marketing shows a non-linear relationship: moderate spend\n",
        "    # can have higher effectiveness per dollar than very high spend due to perceived authenticity\n",
        "    spend_normalized = spend / np.max(spend + 1e-10)\n",
        "\n",
        "    # Authenticity peaks at moderate spend levels (around 0.4-0.6 of max spend)\n",
        "    authenticity_bonus = max_authenticity_bonus * (\n",
        "        1 - np.abs(spend_normalized - 0.5) * 2\n",
        "    ) ** 2\n",
        "\n",
        "    # Apply authenticity effect\n",
        "    authenticity_multiplier = 1 + authenticity_factor * authenticity_bonus\n",
        "    effect = effect * authenticity_multiplier\n",
        "\n",
        "    # When influencer marketing is consistent, it builds credibility over time\n",
        "    rolling_avg = pd.Series(spend).rolling(window=8, min_periods=1).mean().values\n",
        "    consistency_bonus = 0.2 * (1 - np.exp(-rolling_avg / np.mean(spend)))\n",
        "    effect = effect * (1 + consistency_bonus)\n",
        "\n",
        "    # Apply diminishing returns (saturation)\n",
        "    effect = base_effectiveness * np.power(effect, saturation)\n",
        "\n",
        "    return effect\n",
        "\n",
        "# ----- OTHER EFFECT FUNCTIONS (SAME AS ORIGINAL) -----\n",
        "\n",
        "def generate_price_effect(periods=104, base_price=50, price_elasticity=-1.5):\n",
        "    \"\"\"Generate price changes and their effect on sales\"\"\"\n",
        "    # Generate price variations\n",
        "    price_variations = np.random.normal(0, 0.05, periods)\n",
        "    price = base_price * (1 + price_variations)\n",
        "\n",
        "    # Calculate price index (relative to average)\n",
        "    price_index = price / np.mean(price)\n",
        "\n",
        "    # Calculate price effect on sales\n",
        "    price_effect = np.power(price_index, price_elasticity)\n",
        "\n",
        "    return price, price_effect\n",
        "\n",
        "def generate_competitor_effect(periods=104, impact_factor=0.3):\n",
        "    \"\"\"Generate competitor activity effect on sales\"\"\"\n",
        "    # Competitor activities might increase or decrease sales\n",
        "    competitor_effect = np.random.normal(0, impact_factor, periods)\n",
        "    # Make it more smooth with rolling average\n",
        "    competitor_effect = pd.Series(competitor_effect).rolling(window=4, min_periods=1).mean().values\n",
        "\n",
        "    # Convert to multiplicative effect (centered around 1)\n",
        "    competitor_effect = 1 + competitor_effect\n",
        "\n",
        "    return competitor_effect\n",
        "\n",
        "def generate_holiday_effect(periods=104, start_date='2022-01-01'):\n",
        "    \"\"\"Generate holiday effects on sales\"\"\"\n",
        "    dates = pd.date_range(start=start_date, periods=periods, freq='W')\n",
        "    holiday_effect = np.ones(periods)\n",
        "\n",
        "    # Define holidays (simplified for demonstration)\n",
        "    for year in range(2022, 2025):\n",
        "        # Black Friday (4th Thursday in November + following week)\n",
        "        black_friday = pd.Timestamp(f'{year}-11-01') + pd.Timedelta(days=(24-pd.Timestamp(f'{year}-11-01').dayofweek))\n",
        "        bf_week = black_friday.isocalendar()[1]\n",
        "        cyber_week = bf_week + 1\n",
        "\n",
        "        # Christmas\n",
        "        christmas_week = pd.Timestamp(f'{year}-12-25').isocalendar()[1]\n",
        "\n",
        "        # Summer holidays (July)\n",
        "        summer_weeks = [pd.Timestamp(f'{year}-07-{day}').isocalendar()[1] for day in [1, 8, 15, 22]]\n",
        "\n",
        "        # Apply effects\n",
        "        for i, date in enumerate(dates):\n",
        "            week = date.isocalendar()[1]\n",
        "            year_match = date.year == year\n",
        "\n",
        "            if year_match and week == bf_week:\n",
        "                holiday_effect[i] *= 1.8  # Black Friday boost\n",
        "            elif year_match and week == cyber_week:\n",
        "                holiday_effect[i] *= 1.5  # Cyber Week boost\n",
        "            elif year_match and week == christmas_week:\n",
        "                holiday_effect[i] *= 1.7  # Christmas boost\n",
        "            elif year_match and week in summer_weeks:\n",
        "                holiday_effect[i] *= 1.2  # Summer boost\n",
        "\n",
        "    return holiday_effect\n",
        "\n",
        "def generate_weather_effect(periods=104):\n",
        "    \"\"\"Generate weather effects on sales\"\"\"\n",
        "    # Simulate seasonal weather patterns with random variations\n",
        "    seasonal_base = np.sin(np.linspace(0, 4*np.pi, periods))\n",
        "    random_variations = np.random.normal(0, 0.2, periods)\n",
        "    weather_pattern = seasonal_base + random_variations\n",
        "\n",
        "    # Convert to multiplicative effect (centered around 1)\n",
        "    # Assuming both positive and negative weather impacts\n",
        "    weather_effect = 1 + 0.15 * weather_pattern\n",
        "\n",
        "    return weather_effect\n",
        "\n",
        "# ----- EXTENDED COMBINATION FUNCTION WITH SOCIAL & INFLUENCER CHANNELS -----\n",
        "\n",
        "def combine_effects_and_generate_sales(baseline_sales, tv_effect, digital_effect,\n",
        "                                       radio_effect, print_effect, social_media_effect,\n",
        "                                       influencer_effect, price_effect,\n",
        "                                       competitor_effect, holiday_effect, weather_effect,\n",
        "                                       error_std=0.05, channel_interaction=True):\n",
        "    \"\"\"Combine all effects to generate final sales figures including new channels\"\"\"\n",
        "    # New version with smaller coefficients to accommodate more channels\n",
        "    combined_effect = (1 + 0.00015 * tv_effect) * \\\n",
        "                      (1 + 0.00025 * digital_effect) * \\\n",
        "                      (1 + 0.00015 * radio_effect) * \\\n",
        "                      (1 + 0.00008 * print_effect) * \\\n",
        "                      (1 + 0.0003 * social_media_effect) * \\\n",
        "                      (1 + 0.00022 * influencer_effect) * \\\n",
        "                      price_effect * competitor_effect * \\\n",
        "                      holiday_effect * weather_effect\n",
        "\n",
        "    # Add interaction effects between digital, social, and influencer channels\n",
        "    if channel_interaction:\n",
        "        # Digital and Social Media have synergistic effects\n",
        "        digital_social_interaction = 0.00004 * (digital_effect * social_media_effect) / np.mean(digital_effect * social_media_effect)\n",
        "\n",
        "        # Influencer and Social have synergistic effects\n",
        "        influencer_social_interaction = 0.00005 * (influencer_effect * social_media_effect) / np.mean(influencer_effect * social_media_effect)\n",
        "\n",
        "        # Slight negative interaction between TV and digital/social (cannibalization)\n",
        "        tv_digital_interaction = -0.00002 * (tv_effect * (digital_effect + social_media_effect)) / np.mean(tv_effect * (digital_effect + social_media_effect))\n",
        "\n",
        "        # Apply interaction effects\n",
        "        combined_effect = combined_effect * (1 + digital_social_interaction) * \\\n",
        "                                        (1 + influencer_social_interaction) * \\\n",
        "                                        (1 + tv_digital_interaction)\n",
        "\n",
        "    # Generate final sales\n",
        "    sales = baseline_sales * combined_effect\n",
        "\n",
        "    # Add random error\n",
        "    error = np.random.normal(0, error_std, len(sales))\n",
        "    sales = sales * (1 + error)\n",
        "\n",
        "    return sales\n",
        "\n",
        "# ----- MAIN SIMULATION CODE WITH SOCIAL & INFLUENCER CHANNELS -----\n",
        "\n",
        "def generate_mmm_data_extended(periods=104, include_interactions=True):\n",
        "    \"\"\"Generate a complete dataset for MMM analysis with social and influencer channels\"\"\"\n",
        "    # Generate date range\n",
        "    dates = generate_date_range(periods=periods)  # 2 years of weekly data\n",
        "\n",
        "    # Generate baseline sales\n",
        "    baseline_sales = generate_baseline_sales(periods=periods)\n",
        "\n",
        "    # Generate media spend for traditional channels\n",
        "    tv_spend = generate_tv_spend(periods=periods)\n",
        "    digital_spend = generate_digital_spend(periods=periods)\n",
        "    radio_spend = generate_radio_spend(periods=periods)\n",
        "    print_spend = generate_print_spend(periods=periods)\n",
        "\n",
        "    # Generate media spend for new channels\n",
        "    Social_Media_Spend = generate_Social_Media_Spend(periods=periods)\n",
        "    influencer_spend = generate_influencer_spend(periods=periods)\n",
        "\n",
        "    # Calculate media effects for traditional channels\n",
        "    tv_effect = calculate_tv_effect(tv_spend)\n",
        "    digital_effect = calculate_digital_effect(digital_spend)\n",
        "    radio_effect = calculate_radio_effect(radio_spend)\n",
        "    print_effect = calculate_print_effect(print_spend)\n",
        "\n",
        "    # Calculate media effects for new channels\n",
        "    social_media_effect = calculate_social_media_effect(Social_Media_Spend)\n",
        "    influencer_effect = calculate_influencer_effect(influencer_spend)\n",
        "\n",
        "    # Generate other factors\n",
        "    price, price_effect = generate_price_effect(periods=periods)\n",
        "    competitor_effect = generate_competitor_effect(periods=periods)\n",
        "    holiday_effect = generate_holiday_effect(periods=periods)\n",
        "    weather_effect = generate_weather_effect(periods=periods)\n",
        "\n",
        "    # Combine effects to generate sales\n",
        "    sales = combine_effects_and_generate_sales(\n",
        "        baseline_sales, tv_effect, digital_effect, radio_effect, print_effect,\n",
        "        social_media_effect, influencer_effect, price_effect, competitor_effect,\n",
        "        holiday_effect, weather_effect, channel_interaction=include_interactions\n",
        "    )\n",
        "\n",
        "    # Create DataFrame\n",
        "    data = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Sales': sales,\n",
        "        'TV_Spend': tv_spend,\n",
        "        'Digital_Spend': digital_spend,\n",
        "        'Radio_Spend': radio_spend,\n",
        "        'Print_Spend': print_spend,\n",
        "        'Social_Media_Spend': Social_Media_Spend,\n",
        "        'Influencer_Spend': influencer_spend,\n",
        "        'Price': price,\n",
        "        'Holiday_Factor': holiday_effect,\n",
        "        'Competitor_Activity': competitor_effect,\n",
        "        'Weather_Factor': weather_effect,\n",
        "        'Year': [d.year for d in dates],\n",
        "        'Month': [d.month for d in dates],\n",
        "        'Week': [d.isocalendar()[1] for d in dates]\n",
        "    })\n",
        "\n",
        "    # Add time variables\n",
        "    data['WeekNum'] = np.arange(len(data))\n",
        "    data['Sin_Week'] = np.sin(2 * np.pi * data['Week'] / 52)\n",
        "    data['Cos_Week'] = np.cos(2 * np.pi * data['Week'] / 52)\n",
        "\n",
        "    return data\n",
        "\n",
        "# ----- FEATURE ENGINEERING FOR SOCIAL & INFLUENCER CHANNELS -----\n",
        "\n",
        "# Modify the prepare_model_data_extended function to handle potential NaN values\n",
        "\n",
        "def prepare_model_data_extended(data, adstock_params=None, saturation_params=None,\n",
        "                              viral_params=None, authenticity_params=None):\n",
        "    \"\"\"Prepare data for modeling with extended parameters for social and influencer channels\"\"\"\n",
        "    processed_data = data.copy()\n",
        "\n",
        "    # Default adstock parameters if none provided\n",
        "    if adstock_params is None:\n",
        "        adstock_params = {\n",
        "            'TV_Spend': 0.7,\n",
        "            'Digital_Spend': 0.3,\n",
        "            'Radio_Spend': 0.5,\n",
        "            'Print_Spend': 0.4,\n",
        "            'Social_Media_Spend': 0.2,\n",
        "            'Influencer_Spend': 0.4\n",
        "        }\n",
        "\n",
        "    # Default saturation parameters if none provided\n",
        "    if saturation_params is None:\n",
        "        saturation_params = {\n",
        "            'TV_Spend': 0.7,\n",
        "            'Digital_Spend': 0.8,\n",
        "            'Radio_Spend': 0.6,\n",
        "            'Print_Spend': 0.5,\n",
        "            'Social_Media_Spend': 0.75,\n",
        "            'Influencer_Spend': 0.6\n",
        "        }\n",
        "\n",
        "    # Default viral parameters for social media if none provided\n",
        "    if viral_params is None:\n",
        "        viral_params = {\n",
        "            'coefficient': 0.1,\n",
        "            'decay': 0.5\n",
        "        }\n",
        "\n",
        "    # Default authenticity parameters for influencer if none provided\n",
        "    if authenticity_params is None:\n",
        "        authenticity_params = {\n",
        "            'factor': 0.2,\n",
        "            'max_bonus': 0.5\n",
        "        }\n",
        "\n",
        "    # Convert all spend columns to float to avoid dtype issues\n",
        "    for col in ['TV_Spend', 'Digital_Spend', 'Radio_Spend', 'Print_Spend',\n",
        "                'Social_Media_Spend', 'Influencer_Spend']:\n",
        "        if col in processed_data.columns:\n",
        "            processed_data[col] = processed_data[col].astype(float)\n",
        "\n",
        "    # Process traditional channels with standard adstock & saturation\n",
        "    for channel in ['TV_Spend', 'Digital_Spend', 'Radio_Spend', 'Print_Spend']:\n",
        "        # Initialize adstocked spend column\n",
        "        adstock_col = f\"{channel}_Adstocked\"\n",
        "        processed_data[adstock_col] = 0.0  # Use float instead of int\n",
        "\n",
        "        # Apply adstock transformation\n",
        "        processed_data.loc[0, adstock_col] = processed_data.loc[0, channel]\n",
        "        for i in range(1, len(processed_data)):\n",
        "            processed_data.loc[i, adstock_col] = (\n",
        "                processed_data.loc[i, channel] +\n",
        "                adstock_params[channel] * processed_data.loc[i-1, adstock_col]\n",
        "            )\n",
        "\n",
        "        # Apply saturation (diminishing returns)\n",
        "        processed_data[f\"{channel}_Transformed\"] = np.power(\n",
        "            processed_data[adstock_col],\n",
        "            saturation_params[channel]\n",
        "        )\n",
        "\n",
        "    # Process Social Media with viral effects\n",
        "    channel = 'Social_Media_Spend'\n",
        "\n",
        "    # Initialize adstocked spend column\n",
        "    adstock_col = f\"{channel}_Adstocked\"\n",
        "    processed_data[adstock_col] = 0.0  # Use float\n",
        "\n",
        "    # Initialize viral component\n",
        "    viral_col = f\"{channel}_Viral\"\n",
        "    processed_data[viral_col] = 0.0  # Use float\n",
        "\n",
        "    # Apply adstock transformation with viral component\n",
        "    processed_data.loc[0, adstock_col] = processed_data.loc[0, channel]\n",
        "\n",
        "    for i in range(1, len(processed_data)):\n",
        "        # Calculate viral component\n",
        "        new_viral = viral_params['coefficient'] * processed_data.loc[i-1, adstock_col]\n",
        "        processed_data.loc[i, viral_col] = new_viral + viral_params['decay'] * processed_data.loc[i-1, viral_col]\n",
        "\n",
        "        # Basic adstock effect\n",
        "        basic_adstock = processed_data.loc[i, channel] + adstock_params[channel] * processed_data.loc[i-1, adstock_col]\n",
        "\n",
        "        # Combine paid and viral effect\n",
        "        processed_data.loc[i, adstock_col] = basic_adstock + processed_data.loc[i, viral_col]\n",
        "\n",
        "    # Apply network effect with safeguards against division by zero\n",
        "    rolling_spend = processed_data[channel].rolling(window=4, min_periods=1).mean()\n",
        "\n",
        "    # Safely calculate the network multiplier\n",
        "    spend_min = rolling_spend.min()\n",
        "    spend_max = rolling_spend.max()\n",
        "\n",
        "    # If max and min are nearly identical, set a default multiplier to avoid numerical issues\n",
        "    if abs(spend_max - spend_min) < 1e-6:\n",
        "        network_multiplier = np.ones(len(processed_data))\n",
        "    else:\n",
        "        # Use a larger epsilon and ensure we don't divide by zero\n",
        "        epsilon = 1.0  # Larger epsilon to prevent numerical issues\n",
        "        network_multiplier = 1 + (0.2 * (rolling_spend - spend_min) /\n",
        "                                (spend_max - spend_min + epsilon))\n",
        "\n",
        "    processed_data[f\"{channel}_Network\"] = processed_data[adstock_col] * network_multiplier\n",
        "\n",
        "    # Apply saturation to the combined effect\n",
        "    processed_data[f\"{channel}_Transformed\"] = np.power(\n",
        "        processed_data[f\"{channel}_Network\"],\n",
        "        saturation_params[channel]\n",
        "    )\n",
        "\n",
        "    # Process Influencer with authenticity effects\n",
        "    channel = 'Influencer_Spend'\n",
        "\n",
        "    # Initialize adstocked spend column\n",
        "    adstock_col = f\"{channel}_Adstocked\"\n",
        "    processed_data[adstock_col] = 0.0  # Use float\n",
        "\n",
        "    # Apply adstock transformation\n",
        "    processed_data.loc[0, adstock_col] = processed_data.loc[0, channel]\n",
        "    for i in range(1, len(processed_data)):\n",
        "        processed_data.loc[i, adstock_col] = (\n",
        "            processed_data.loc[i, channel] +\n",
        "            adstock_params[channel] * processed_data.loc[i-1, adstock_col]\n",
        "        )\n",
        "\n",
        "    # Calculate authenticity bonus with safeguards\n",
        "    channel_max = processed_data[channel].max()\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if channel_max == 0:\n",
        "        spend_normalized = np.zeros(len(processed_data))\n",
        "    else:\n",
        "        spend_normalized = processed_data[channel] / (channel_max + 1e-6)\n",
        "\n",
        "    # Authenticity peaks at moderate spend levels\n",
        "    authenticity_bonus = authenticity_params['max_bonus'] * (\n",
        "        1 - np.abs(spend_normalized - 0.5) * 2\n",
        "    ) ** 2\n",
        "\n",
        "    # Apply authenticity effect\n",
        "    authenticity_multiplier = 1 + authenticity_params['factor'] * authenticity_bonus\n",
        "    processed_data[f\"{channel}_Authentic\"] = processed_data[adstock_col] * authenticity_multiplier\n",
        "\n",
        "    # Apply consistency bonus safely\n",
        "    rolling_avg = processed_data[channel].rolling(window=8, min_periods=1).mean()\n",
        "\n",
        "    # Avoid division by zero in mean calculation\n",
        "    channel_mean = processed_data[channel].mean()\n",
        "    if channel_mean == 0:\n",
        "        consistency_bonus = np.zeros(len(processed_data))\n",
        "    else:\n",
        "        consistency_bonus = 0.2 * (1 - np.exp(-rolling_avg / (channel_mean + 1e-6)))\n",
        "\n",
        "    processed_data[f\"{channel}_Consistent\"] = processed_data[f\"{channel}_Authentic\"] * (1 + consistency_bonus)\n",
        "\n",
        "    # Apply saturation to the final effect\n",
        "    processed_data[f\"{channel}_Transformed\"] = np.power(\n",
        "        processed_data[f\"{channel}_Consistent\"],\n",
        "        saturation_params[channel]\n",
        "    )\n",
        "\n",
        "    # Create interaction terms for digital, social and influencer\n",
        "    processed_data['Digital_Social_Interaction'] = (\n",
        "        processed_data['Digital_Spend_Transformed'] *\n",
        "        processed_data['Social_Media_Spend_Transformed']\n",
        "    ) / 1e6\n",
        "\n",
        "    processed_data['Social_Influencer_Interaction'] = (\n",
        "        processed_data['Social_Media_Spend_Transformed'] *\n",
        "        processed_data['Influencer_Spend_Transformed']\n",
        "    ) / 1e6\n",
        "\n",
        "    processed_data['TV_Digital_Interaction'] = (\n",
        "        processed_data['TV_Spend_Transformed'] *\n",
        "        processed_data['Digital_Spend_Transformed']\n",
        "    ) / 1e6\n",
        "\n",
        "    # Create log transformed variables for all channels\n",
        "    for channel in ['TV_Spend', 'Digital_Spend', 'Radio_Spend', 'Print_Spend',\n",
        "                   'Social_Media_Spend', 'Influencer_Spend']:\n",
        "        # Add small constant to avoid log(0)\n",
        "        processed_data[f'Log_{channel}'] = np.log1p(processed_data[channel])\n",
        "\n",
        "    # Create squared terms for price (non-linear effects)\n",
        "    processed_data['Price_Squared'] = processed_data['Price'] ** 2\n",
        "\n",
        "    # Final check for NaN values and fill them\n",
        "    if processed_data.isna().any().any():\n",
        "        # Print columns with NaN values for debugging\n",
        "        nan_columns = processed_data.columns[processed_data.isna().any()].tolist()\n",
        "        print(f\"Warning: NaN values found in columns: {nan_columns}\")\n",
        "\n",
        "        # Fill NaN values with appropriate methods\n",
        "        # For numeric columns, fill with median or 0\n",
        "        numeric_cols = processed_data.select_dtypes(include=['number']).columns\n",
        "        for col in numeric_cols:\n",
        "            if processed_data[col].isna().any():\n",
        "                if col.endswith('_Transformed') or col.endswith('_Adstocked'):\n",
        "                    # For derived variables, use 0 as a safe value\n",
        "                    processed_data[col] = processed_data[col].fillna(0)\n",
        "                else:\n",
        "                    # For other numeric columns, use median\n",
        "                    processed_data[col] = processed_data[col].fillna(processed_data[col].median())\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Add a fix for the train_model_extended function as well\n",
        "def train_model_extended(data, model_type='ridge', test_size=26, feature_selection='transformed', **kwargs):\n",
        "    \"\"\"Train a marketing mix model with specified parameters for extended channel set\"\"\"\n",
        "    # Prepare features\n",
        "    features = prepare_features_extended(data, feature_selection)\n",
        "\n",
        "    # Split data into training and testing\n",
        "    train_data = data.iloc[:-test_size].copy()\n",
        "    test_data = data.iloc[-test_size:].copy()\n",
        "\n",
        "    # Prepare data\n",
        "    X_train = train_data[features]\n",
        "    y_train = train_data['Sales']\n",
        "    X_test = test_data[features]\n",
        "    y_test = test_data['Sales']\n",
        "\n",
        "    # Check for NaN values before model training\n",
        "    if X_train.isna().any().any() or y_train.isna().any():\n",
        "        print(\"Warning: NaN values found in training data. Filling them...\")\n",
        "        # Fill NaN values in X_train\n",
        "        X_train = X_train.fillna(X_train.median())\n",
        "        # Fill NaN values in y_train\n",
        "        y_train = y_train.fillna(y_train.median())\n",
        "\n",
        "    if X_test.isna().any().any() or y_test.isna().any():\n",
        "        print(\"Warning: NaN values found in test data. Filling them...\")\n",
        "        # Fill NaN values in X_test\n",
        "        X_test = X_test.fillna(X_train.median())  # Use training median\n",
        "        # Fill NaN values in y_test\n",
        "        y_test = y_test.fillna(y_train.median())  # Use training median\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train model based on type\n",
        "    if model_type == 'ols':\n",
        "        # Add constant for statsmodels\n",
        "        X_train_sm = sm.add_constant(X_train_scaled)\n",
        "        X_test_sm = sm.add_constant(X_test_scaled)\n",
        "\n",
        "        # Train model\n",
        "        model = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "        # Make predictions\n",
        "        train_pred = model.predict(X_train_sm)\n",
        "        test_pred = model.predict(X_test_sm)\n",
        "\n",
        "    elif model_type == 'ridge':\n",
        "        alpha = kwargs.get('alpha', 1.0)\n",
        "        model = Ridge(alpha=alpha)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    elif model_type == 'lasso':\n",
        "        alpha = kwargs.get('alpha', 0.1)\n",
        "        model = Lasso(alpha=alpha)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    elif model_type == 'elasticnet':\n",
        "        alpha = kwargs.get('alpha', 0.1)\n",
        "        l1_ratio = kwargs.get('l1_ratio', 0.5)\n",
        "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
        "\n",
        "    results = {\n",
        "        'model': model,\n",
        "        'model_type': model_type,\n",
        "        'train_pred': train_pred,\n",
        "        'test_pred': test_pred,\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'train_rmse': train_rmse,\n",
        "        'test_rmse': test_rmse,\n",
        "        'features': features,\n",
        "        'feature_selection': feature_selection,\n",
        "        'scaler': scaler,\n",
        "        'train_data': train_data,\n",
        "        'test_data': test_data\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ----- EXTENDED ANALYSIS FUNCTIONS FOR SOCIAL & INFLUENCER -----\n",
        "\n",
        "def calculate_roi_extended(model_results, data):\n",
        "    \"\"\"Calculate ROI for each marketing channel including social & influencer\"\"\"\n",
        "    model = model_results['model']\n",
        "    model_type = model_results['model_type']\n",
        "    features = model_results['features']\n",
        "    feature_selection = model_results['feature_selection']\n",
        "    scaler = model_results['scaler']\n",
        "\n",
        "    # Get coefficients based on model type\n",
        "    if model_type == 'ols':\n",
        "        coef_dict = model.params.to_dict()\n",
        "        if 'const' in coef_dict:\n",
        "            del coef_dict['const']\n",
        "    else:\n",
        "        coef_dict = {}\n",
        "        for i, feature in enumerate(features):\n",
        "            coef_dict[feature] = model.coef_[i]\n",
        "\n",
        "    # Calculate channel spend totals\n",
        "    channel_spend = {\n",
        "        'TV': data['TV_Spend'].sum(),\n",
        "        'Digital': data['Digital_Spend'].sum(),\n",
        "        'Radio': data['Radio_Spend'].sum(),\n",
        "        'Print': data['Print_Spend'].sum(),\n",
        "        'Social_Media': data['Social_Media_Spend'].sum(),\n",
        "        'Influencer': data['Influencer_Spend'].sum()\n",
        "    }\n",
        "\n",
        "    # Calculate average sales\n",
        "    avg_sales = data['Sales'].mean()\n",
        "\n",
        "    # Calculate ROI for each channel\n",
        "    roi_results = {}\n",
        "\n",
        "    # Map feature prefixes to channel names\n",
        "    channel_prefixes = {\n",
        "        'TV': ['TV_Spend', 'TV_Spend_Adstocked', 'TV_Spend_Transformed', 'Log_TV_Spend'],\n",
        "        'Digital': ['Digital_Spend', 'Digital_Spend_Adstocked', 'Digital_Spend_Transformed', 'Log_Digital_Spend'],\n",
        "        'Radio': ['Radio_Spend', 'Radio_Spend_Adstocked', 'Radio_Spend_Transformed', 'Log_Radio_Spend'],\n",
        "        'Print': ['Print_Spend', 'Print_Spend_Adstocked', 'Print_Spend_Transformed', 'Log_Print_Spend'],\n",
        "        'Social_Media': ['Social_Media_Spend', 'Social_Media_Spend_Adstocked', 'Social_Media_Spend_Network',\n",
        "                        'Social_Media_Spend_Transformed', 'Social_Media_Spend_Viral', 'Log_Social_Media_Spend'],\n",
        "        'Influencer': ['Influencer_Spend', 'Influencer_Spend_Adstocked', 'Influencer_Spend_Authentic',\n",
        "                      'Influencer_Spend_Consistent', 'Influencer_Spend_Transformed', 'Log_Influencer_Spend']\n",
        "    }\n",
        "\n",
        "    # For each channel, find the corresponding feature in the model\n",
        "    for channel, prefixes in channel_prefixes.items():\n",
        "        # Find the feature that corresponds to this channel\n",
        "        channel_features = []\n",
        "        for feature in features:\n",
        "            for prefix in prefixes:\n",
        "                if feature.startswith(prefix) and not any(inter in feature for inter in ['Interaction']):\n",
        "                    channel_features.append(feature)\n",
        "\n",
        "        # If no features found for this channel, skip\n",
        "        if not channel_features:\n",
        "            roi_results[channel] = 0\n",
        "            continue\n",
        "\n",
        "        # Get the coefficient\n",
        "        channel_coef = 0\n",
        "        for feature in channel_features:\n",
        "            channel_coef += coef_dict.get(feature, 0)\n",
        "\n",
        "        # Calculate ROI\n",
        "        channel_key = channel if channel != 'Social_Media' and channel != 'Influencer' else channel.lower()\n",
        "        channel_total_spend = channel_spend[channel]\n",
        "        if channel_total_spend > 0:\n",
        "            roi = (channel_coef * avg_sales * len(data)) / channel_total_spend\n",
        "            roi_results[channel] = max(0, roi)  # Ensure non-negative ROI\n",
        "        else:\n",
        "            roi_results[channel] = 0\n",
        "\n",
        "    # Add interaction effects to ROI\n",
        "    # For Digital and Social Media interaction\n",
        "    if 'Digital_Social_Interaction' in features:\n",
        "        interaction_feature = 'Digital_Social_Interaction'\n",
        "        interaction_coef = coef_dict.get(interaction_feature, 0)\n",
        "\n",
        "        # Allocate interaction effect proportionally to Digital and Social\n",
        "        digital_spend = channel_spend['Digital']\n",
        "        social_spend = channel_spend['Social_Media']\n",
        "        total_spend = digital_spend + social_spend\n",
        "\n",
        "        if total_spend > 0:\n",
        "            digital_share = digital_spend / total_spend\n",
        "            social_share = social_spend / total_spend\n",
        "\n",
        "            interaction_effect = interaction_coef * avg_sales * len(data)\n",
        "\n",
        "            # Add to ROI\n",
        "            if digital_spend > 0:\n",
        "                roi_results['Digital'] += (interaction_effect * digital_share) / digital_spend\n",
        "\n",
        "            if social_spend > 0:\n",
        "                roi_results['Social_Media'] += (interaction_effect * social_share) / social_spend\n",
        "\n",
        "    # For Social Media and Influencer interaction\n",
        "    if 'Social_Influencer_Interaction' in features:\n",
        "        interaction_feature = 'Social_Influencer_Interaction'\n",
        "        interaction_coef = coef_dict.get(interaction_feature, 0)\n",
        "\n",
        "        # Allocate interaction effect proportionally\n",
        "        social_spend = channel_spend['Social_Media']\n",
        "        influencer_spend = channel_spend['Influencer']\n",
        "        total_spend = social_spend + influencer_spend\n",
        "\n",
        "        if total_spend > 0:\n",
        "            social_share = social_spend / total_spend\n",
        "            influencer_share = influencer_spend / total_spend\n",
        "\n",
        "            interaction_effect = interaction_coef * avg_sales * len(data)\n",
        "\n",
        "            # Add to ROI\n",
        "            if social_spend > 0:\n",
        "                roi_results['Social_Media'] += (interaction_effect * social_share) / social_spend\n",
        "\n",
        "            if influencer_spend > 0:\n",
        "                roi_results['Influencer'] += (interaction_effect * influencer_share) / influencer_spend\n",
        "\n",
        "    return roi_results\n",
        "\n",
        "def decompose_sales_extended(model_results, data):\n",
        "    \"\"\"Decompose sales into contributions from different factors including social & influencer\"\"\"\n",
        "    model = model_results['model']\n",
        "    model_type = model_results['model_type']\n",
        "    features = model_results['features']\n",
        "    scaler = model_results['scaler']\n",
        "\n",
        "    # Group features by category\n",
        "    feature_groups = {\n",
        "        'TV': [f for f in features if ('TV_Spend' in f) and not any(inter in f for inter in ['Interaction'])],\n",
        "        'Digital': [f for f in features if ('Digital_Spend' in f) and not any(inter in f for inter in ['Interaction'])],\n",
        "        'Radio': [f for f in features if 'Radio_Spend' in f],\n",
        "        'Print': [f for f in features if 'Print_Spend' in f],\n",
        "        'Social_Media': [f for f in features if ('Social_Media_Spend' in f) and not any(inter in f for inter in ['Interaction'])],\n",
        "        'Influencer': [f for f in features if ('Influencer_Spend' in f) and not any(inter in f for inter in ['Interaction'])],\n",
        "        'Price': [f for f in features if 'Price' in f],\n",
        "        'Seasonality': ['Sin_Week', 'Cos_Week'],\n",
        "        'External Factors': ['Holiday_Factor', 'Competitor_Activity', 'Weather_Factor'],\n",
        "        'Interactions': [f for f in features if 'Interaction' in f]\n",
        "    }\n",
        "\n",
        "    # Prepare data for prediction\n",
        "    X = data[features]\n",
        "    X_scaled = scaler.transform(X)\n",
        "\n",
        "    contrib_dict = {}\n",
        "\n",
        "    # For statsmodels OLS\n",
        "    if model_type == 'ols':\n",
        "        X_with_const = sm.add_constant(X_scaled)\n",
        "\n",
        "        # Calculate contribution for each feature group\n",
        "        for group, group_features in feature_groups.items():\n",
        "            group_contrib = np.zeros(len(data))\n",
        "            for feature in group_features:\n",
        "                if feature in model.params.index:\n",
        "                    feature_idx = list(model.params.index).index(feature)\n",
        "                    feature_contrib = X_with_const[:, feature_idx] * model.params[feature]\n",
        "                    group_contrib += feature_contrib\n",
        "            contrib_dict[group] = group_contrib\n",
        "\n",
        "        # Add baseline/intercept\n",
        "        if 'const' in model.params.index:\n",
        "            contrib_dict['Baseline'] = np.ones(len(data)) * model.params['const']\n",
        "        else:\n",
        "            contrib_dict['Baseline'] = np.zeros(len(data))\n",
        "\n",
        "    # For sklearn models (Ridge, Lasso, ElasticNet)\n",
        "    else:\n",
        "        # Calculate contribution for each feature group\n",
        "        for group, group_features in feature_groups.items():\n",
        "            group_contrib = np.zeros(len(data))\n",
        "            for feature in group_features:\n",
        "                if feature in features:\n",
        "                    feature_idx = features.index(feature)\n",
        "                    feature_contrib = X_scaled[:, feature_idx] * model.coef_[feature_idx]\n",
        "                    group_contrib += feature_contrib\n",
        "            contrib_dict[group] = group_contrib\n",
        "\n",
        "        # Add baseline/intercept\n",
        "        if hasattr(model, 'intercept_'):\n",
        "            contrib_dict['Baseline'] = np.ones(len(data)) * model.intercept_\n",
        "        else:\n",
        "            contrib_dict['Baseline'] = np.zeros(len(data))\n",
        "\n",
        "    return contrib_dict\n",
        "\n",
        "# Fix for the KeyError in the simulate_budget_allocation_extended function\n",
        "\n",
        "def simulate_budget_allocation_extended(model_results, data, budget_total, n_simulations=100, constraints=None):\n",
        "    \"\"\"Simulate different budget allocations to optimize sales with social & influencer channels\"\"\"\n",
        "    model = model_results['model']\n",
        "    model_type = model_results['model_type']\n",
        "    features = model_results['features']\n",
        "    feature_selection = model_results['feature_selection']\n",
        "    scaler = model_results['scaler']\n",
        "\n",
        "    # Original media spend\n",
        "    original_spend = {\n",
        "        'TV': data['TV_Spend'].sum(),\n",
        "        'Digital': data['Digital_Spend'].sum(),\n",
        "        'Radio': data['Radio_Spend'].sum(),\n",
        "        'Print': data['Print_Spend'].sum(),\n",
        "        'Social_Media': data['Social_Media_Spend'].sum(),\n",
        "        'Influencer': data['Influencer_Spend'].sum()\n",
        "    }\n",
        "\n",
        "    total_original = sum(original_spend.values())\n",
        "\n",
        "    # Define channels\n",
        "    channels = ['TV', 'Digital', 'Radio', 'Print', 'Social_Media', 'Influencer']\n",
        "\n",
        "    # Define corresponding column names in the DataFrame\n",
        "    # This is the key fix - explicitly mapping channel names to column names\n",
        "    column_map = {\n",
        "    'TV': 'TV_Spend',\n",
        "    'Digital': 'Digital_Spend',\n",
        "    'Radio': 'Radio_Spend',\n",
        "    'Print': 'Print_Spend',\n",
        "    'Social_Media': 'Social_Media_Spend',\n",
        "    'Influencer': 'Influencer_Spend'\n",
        "}\n",
        "\n",
        "\n",
        "    # Apply constraints if provided\n",
        "    if constraints is None:\n",
        "        constraints = {}\n",
        "\n",
        "    min_pct = {channel: constraints.get(f'min_{channel.lower()}', 0.0) for channel in channels}\n",
        "    max_pct = {channel: constraints.get(f'max_{channel.lower()}', 1.0) for channel in channels}\n",
        "\n",
        "    simulation_results = []\n",
        "\n",
        "    # Create random allocations\n",
        "    for _ in range(n_simulations):\n",
        "        retry_count = 0\n",
        "        max_retries = 50\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            # Generate random weights\n",
        "            weights = np.random.random(len(channels))\n",
        "            weights = weights / weights.sum()\n",
        "\n",
        "            # Check if weights satisfy constraints\n",
        "            valid_allocation = True\n",
        "            for i, channel in enumerate(channels):\n",
        "                if weights[i] < min_pct[channel] or weights[i] > max_pct[channel]:\n",
        "                    valid_allocation = False\n",
        "                    break\n",
        "\n",
        "            if valid_allocation:\n",
        "                break\n",
        "\n",
        "            retry_count += 1\n",
        "\n",
        "        # If we couldn't find a valid allocation after max retries, use a simple approach\n",
        "        if not valid_allocation:\n",
        "            # Start with minimum allocations\n",
        "            weights = np.array([min_pct[channel] for channel in channels])\n",
        "            # Distribute remaining budget proportionally\n",
        "            remaining = 1.0 - sum(weights)\n",
        "            if remaining > 0:\n",
        "                # Calculate the range between min and max for each channel\n",
        "                ranges = np.array([max_pct[channel] - min_pct[channel] for channel in channels])\n",
        "                # Normalize the ranges\n",
        "                if sum(ranges) > 0:\n",
        "                    normalized_ranges = ranges / sum(ranges)\n",
        "                    # Distribute remaining budget\n",
        "                    weights += normalized_ranges * remaining\n",
        "\n",
        "        # Calculate new budget allocation\n",
        "        allocation = {}\n",
        "        for i, channel in enumerate(channels):\n",
        "            allocation[channel] = budget_total * weights[i]\n",
        "\n",
        "        # Use this allocation to predict sales\n",
        "        new_data = data.copy()\n",
        "\n",
        "        # Apply allocation - Using the column map to get the correct column names\n",
        "        for channel in channels:\n",
        "            channel_col = column_map[channel]  # Use the column map here\n",
        "            channel_scaling = allocation[channel] / original_spend[channel] if original_spend[channel] > 0 else 0\n",
        "            new_data[channel_col] = data[channel_col] * channel_scaling\n",
        "\n",
        "        # Recalculate derived variables based on feature selection\n",
        "        if feature_selection == 'transformed':\n",
        "            # Process the data with our extended transformation function\n",
        "            viral_params = {\n",
        "                'coefficient': 0.1,\n",
        "                'decay': 0.5\n",
        "            }\n",
        "            authenticity_params = {\n",
        "                'factor': 0.2,\n",
        "                'max_bonus': 0.5\n",
        "            }\n",
        "\n",
        "            new_data = prepare_model_data_extended(\n",
        "                new_data,\n",
        "                viral_params=viral_params,\n",
        "                authenticity_params=authenticity_params\n",
        "            )\n",
        "        elif feature_selection == 'log':\n",
        "            # Just recalculate log variables\n",
        "            for channel in channels:\n",
        "                channel_col = column_map[channel]  # Use the column map here\n",
        "                new_data[f'Log_{channel_col}'] = np.log1p(new_data[channel_col])\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        X_new = new_data[features]\n",
        "        X_new_scaled = scaler.transform(X_new)\n",
        "\n",
        "        # Predict sales\n",
        "        if model_type == 'ols':\n",
        "            X_new_with_const = sm.add_constant(X_new_scaled)\n",
        "            predicted_sales = model.predict(X_new_with_const)\n",
        "        else:\n",
        "            predicted_sales = model.predict(X_new_scaled)\n",
        "\n",
        "        total_sales = np.sum(predicted_sales)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'allocation': allocation,\n",
        "            'total_sales': total_sales,\n",
        "            'allocation_percentages': {k: v/budget_total*100 for k, v in allocation.items()}\n",
        "        }\n",
        "\n",
        "        simulation_results.append(result)\n",
        "\n",
        "    # Sort results by total sales\n",
        "    simulation_results.sort(key=lambda x: x['total_sales'], reverse=True)\n",
        "\n",
        "    return simulation_results\n",
        "\n",
        "# ----- EXTENDED VISUALIZATION FUNCTIONS FOR SOCIAL & INFLUENCER -----\n",
        "\n",
        "def plot_media_spend_patterns_extended(data):\n",
        "    \"\"\"Create a Plotly figure showing media spend patterns over time for all channels\"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add traces for traditional channels\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['TV_Spend'],\n",
        "        name='TV',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['Digital_Spend'],\n",
        "        name='Digital',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['Radio_Spend'],\n",
        "        name='Radio',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['Print_Spend'],\n",
        "        name='Print',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "\n",
        "    # Add traces for new channels\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['Social_Media_Spend'],\n",
        "        name='Social Media',\n",
        "        line=dict(width=2, dash='dot')\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['Influencer_Spend'],\n",
        "        name='Influencer',\n",
        "        line=dict(width=2, dash='dot')\n",
        "    ))\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title='Media Spend Patterns Over Time - All Channels',\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Spend ($)',\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        ),\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def plot_roi_comparison_extended(roi_results, title='Channel ROI Comparison'):\n",
        "    \"\"\"Create a Plotly figure showing ROI comparison across channels including social & influencer\"\"\"\n",
        "    channels = list(roi_results.keys())\n",
        "    roi_values = [roi_results[ch] for ch in channels]\n",
        "\n",
        "    # Define colors based on ROI value\n",
        "    colors = []\n",
        "    for roi in roi_values:\n",
        "        if roi > 2.0:\n",
        "            colors.append('#1a9850')  # Dark green for very high ROI\n",
        "        elif roi > 1.5:\n",
        "            colors.append('#66bd63')  # Green for high ROI\n",
        "        elif roi > 1.0:\n",
        "            colors.append('#a6d96a')  # Light green for good ROI\n",
        "        elif roi > 0.5:\n",
        "            colors.append('#fee08b')  # Yellow for moderate ROI\n",
        "        else:\n",
        "            colors.append('#fdae61')  # Orange for low ROI\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add bar chart\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=channels,\n",
        "        y=roi_values,\n",
        "        marker_color=colors,\n",
        "        text=[f\"{roi:.2f}\" for roi in roi_values],\n",
        "        textposition='outside'\n",
        "    ))\n",
        "\n",
        "    # Add horizontal line at ROI = 1.0\n",
        "    fig.add_shape(\n",
        "        type='line',\n",
        "        x0=-0.5,\n",
        "        y0=1.0,\n",
        "        x1=len(channels)-0.5,\n",
        "        y1=1.0,\n",
        "        line=dict(\n",
        "            color='red',\n",
        "            width=2,\n",
        "            dash='dash'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Add annotation for break-even line\n",
        "    fig.add_annotation(\n",
        "        x=channels[-1],\n",
        "        y=1.0,\n",
        "        text=\"Break-even (ROI=1.0)\",\n",
        "        showarrow=False,\n",
        "        yshift=5,\n",
        "        font=dict(color='red')\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title='Channel',\n",
        "        yaxis_title='ROI (Return on Ad Spend)',\n",
        "        template=\"plotly_white\",\n",
        "        yaxis=dict(\n",
        "            range=[0, max(max(roi_values) * 1.1, 1.2)]  # Add some headroom for text\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def plot_channel_response_curves_extended(data, model_results, max_multiplier=2.0):\n",
        "    \"\"\"Create Plotly figures showing response curves for all channels including social & influencer\"\"\"\n",
        "    model = model_results['model']\n",
        "    model_type = model_results['model_type']\n",
        "    features = model_results['features']\n",
        "    feature_selection = model_results['feature_selection']\n",
        "    scaler = model_results['scaler']\n",
        "\n",
        "    # All channels including new ones\n",
        "    channels = ['TV', 'Digital', 'Radio', 'Print', 'Social_Media', 'Influencer']\n",
        "\n",
        "    # Define column mapping for consistent reference\n",
        "    column_map = {\n",
        "        'TV': 'TV_Spend',\n",
        "        'Digital': 'Digital_Spend',\n",
        "        'Radio': 'Radio_Spend',\n",
        "        'Print': 'Print_Spend',\n",
        "        'Social_Media': 'Social_Media_Spend',\n",
        "        'Influencer': 'Influencer_Spend'\n",
        "    }\n",
        "\n",
        "    # Create a subplot with 3x2 grid\n",
        "    fig = make_subplots(\n",
        "        rows=3,\n",
        "        cols=2,\n",
        "        subplot_titles=[f\"{channel} Response Curve\" for channel in channels]\n",
        "    )\n",
        "\n",
        "    for i, channel in enumerate(channels):\n",
        "        row = (i // 2) + 1\n",
        "        col = (i % 2) + 1\n",
        "\n",
        "        # Get the appropriate spend column name using the column map\n",
        "        spend_col = column_map[channel]\n",
        "\n",
        "        # Create range of spend multipliers\n",
        "        multipliers = np.linspace(0, max_multiplier, 20)\n",
        "        predicted_sales = []\n",
        "\n",
        "        for mult in multipliers:\n",
        "            # Create a copy of the data with modified spend\n",
        "            test_data = data.copy()\n",
        "\n",
        "            # Modify spend for this channel only\n",
        "            test_data[spend_col] = data[spend_col] * mult\n",
        "\n",
        "            # Recalculate derived variables based on feature selection\n",
        "            if feature_selection == 'transformed':\n",
        "                # Process the data with our extended transformation function\n",
        "                viral_params = {\n",
        "                    'coefficient': 0.1,\n",
        "                    'decay': 0.5\n",
        "                }\n",
        "                authenticity_params = {\n",
        "                    'factor': 0.2,\n",
        "                    'max_bonus': 0.5\n",
        "                }\n",
        "\n",
        "                test_data = prepare_model_data_extended(\n",
        "                    test_data,\n",
        "                    viral_params=viral_params,\n",
        "                    authenticity_params=authenticity_params\n",
        "                )\n",
        "            elif feature_selection == 'log':\n",
        "                # Just recalculate log variables\n",
        "                test_data[f'Log_{spend_col}'] = np.log1p(test_data[spend_col])\n",
        "\n",
        "            # Prepare features for prediction\n",
        "            X_test = test_data[features]\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            # Make prediction\n",
        "            if model_type == 'ols':\n",
        "                X_test_with_const = sm.add_constant(X_test_scaled)\n",
        "                y_pred = model.predict(X_test_with_const)\n",
        "            else:\n",
        "                y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "            predicted_sales.append(np.sum(y_pred))\n",
        "\n",
        "        # Convert to percentage changes\n",
        "        base_sales = predicted_sales[0]  # Sales with zero spend\n",
        "        pct_change = [(s - base_sales) / base_sales * 100 if base_sales > 0 else 0 for s in predicted_sales]\n",
        "\n",
        "        # Calculate average original spend\n",
        "        avg_spend = data[spend_col].mean()\n",
        "        total_spend = [avg_spend * m * len(data) for m in multipliers]\n",
        "\n",
        "        # Add line trace for response curve\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=total_spend,\n",
        "                y=pct_change,\n",
        "                mode='lines',\n",
        "                name=f\"{channel} Response\",\n",
        "                line=dict(color='blue', width=2),\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=row,\n",
        "            col=col\n",
        "        )\n",
        "\n",
        "        # Add point for current spend level\n",
        "        current_idx = 10  # Assuming 20 points and 2.0 max multiplier, 1.0 is at index 10\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=[total_spend[current_idx]],\n",
        "                y=[pct_change[current_idx]],\n",
        "                mode='markers',\n",
        "                marker=dict(color='red', size=10),\n",
        "                name=f\"Current {channel} Spend\",\n",
        "                text=f\"Current Spend: ${total_spend[current_idx]/1e3:.0f}k\",\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=row,\n",
        "            col=col\n",
        "        )\n",
        "\n",
        "        # Calculate and plot optimal point (where marginal returns start diminishing significantly)\n",
        "        # Use second derivative approach\n",
        "        y = np.array(pct_change)\n",
        "        grad = np.gradient(np.gradient(y))\n",
        "        # Find where the acceleration drops below threshold\n",
        "        threshold = 0.1 * np.min(grad) if np.min(grad) < 0 else -0.01  # 10% of min gradient\n",
        "        optimal_idx = np.where(grad < threshold)[0]\n",
        "        if len(optimal_idx) > 0:\n",
        "            optimal_idx = optimal_idx[0]\n",
        "            if optimal_idx > 0 and optimal_idx < len(multipliers) - 1:  # Ensure we're not picking the endpoints\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=[total_spend[optimal_idx]],\n",
        "                        y=[pct_change[optimal_idx]],\n",
        "                        mode='markers',\n",
        "                        marker=dict(color='green', size=10),\n",
        "                        name=f\"Optimal {channel} Spend\",\n",
        "                        text=f\"Optimal Spend: ${total_spend[optimal_idx]/1e3:.0f}k\",\n",
        "                        showlegend=False\n",
        "                    ),\n",
        "                    row=row,\n",
        "                    col=col\n",
        "                )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title='Channel Response Curves - Sales Lift vs Spend',\n",
        "        template=\"plotly_white\",\n",
        "        height=800,\n",
        "        width=1000\n",
        "    )\n",
        "\n",
        "    # Update x and y-axis titles for all subplots\n",
        "    for i in range(1, 4):  # rows\n",
        "        for j in range(1, 3):  # columns\n",
        "            fig.update_xaxes(title_text=\"Total Spend ($)\", row=i, col=j)\n",
        "            fig.update_yaxes(title_text=\"Sales Lift (%)\", row=i, col=j)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def plot_sales_decomposition_extended(contrib_dict, data, title='Sales Decomposition'):\n",
        "    \"\"\"Create a Plotly figure showing sales decomposition over time including social & influencer\"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Get all contribution components\n",
        "    components = list(contrib_dict.keys())\n",
        "\n",
        "    # Create stacked area chart\n",
        "    for component in components:\n",
        "        if component != 'Baseline':  # Add everything except baseline first\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data['Date'],\n",
        "                y=contrib_dict[component],\n",
        "                mode='lines',\n",
        "                stackgroup='one',\n",
        "                name=component,\n",
        "                hoverinfo='x+y+name'\n",
        "            ))\n",
        "\n",
        "    # Add baseline as a line beneath everything\n",
        "    if 'Baseline' in components:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=data['Date'],\n",
        "            y=contrib_dict['Baseline'],\n",
        "            mode='lines',\n",
        "            name='Baseline',\n",
        "            line=dict(color='black', width=2),\n",
        "            hoverinfo='x+y+name'\n",
        "        ))\n",
        "\n",
        "    # Add actual sales\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data['Date'],\n",
        "        y=data['Sales'],\n",
        "        mode='lines',\n",
        "        name='Actual Sales',\n",
        "        line=dict(color='red', width=2),\n",
        "        hoverinfo='x+y+name'\n",
        "    ))\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Sales Contribution',\n",
        "        template=\"plotly_white\",\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def plot_budget_optimization_results(simulation_results, top_n=5):\n",
        "    \"\"\"Create a Plotly figure showing top budget allocation strategies\"\"\"\n",
        "    # Get top N results\n",
        "    top_results = simulation_results[:top_n]\n",
        "\n",
        "    # Extract channels\n",
        "    channels = list(top_results[0]['allocation_percentages'].keys())\n",
        "\n",
        "    # Create figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add current strategy for comparison (assuming first simulation is current)\n",
        "    current_allocations = list(top_results[0]['allocation_percentages'].values())\n",
        "    current_sales = top_results[0]['total_sales']\n",
        "\n",
        "    bar_width = 0.15\n",
        "\n",
        "    # Add a bar for each top result\n",
        "    for i, result in enumerate(top_results):\n",
        "        allocations = [result['allocation_percentages'][ch] for ch in channels]\n",
        "        sales_increase = (result['total_sales'] - current_sales) / current_sales * 100\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=channels,\n",
        "            y=allocations,\n",
        "            name=f\"Strategy {i+1}: +{sales_increase:.1f}%\",\n",
        "            width=bar_width,\n",
        "            offset=(i - len(top_results)/2 + 0.5) * bar_width\n",
        "        ))\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title='Top Budget Allocation Strategies',\n",
        "        xaxis_title='Channel',\n",
        "        yaxis_title='Budget Allocation (%)',\n",
        "        barmode='group',\n",
        "        template=\"plotly_white\",\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# ----- MAIN EXECUTION FUNCTION -----\n",
        "\n",
        "def run_mmm_analysis(periods=104, train_model=True, visualize=True):\n",
        "    \"\"\"Run the complete MMM analysis pipeline with social and influencer channels\"\"\"\n",
        "    print(\"Generating MMM data...\")\n",
        "    data = generate_mmm_data_extended(periods=periods)\n",
        "\n",
        "    if train_model:\n",
        "        print(\"Processing data for modeling...\")\n",
        "        processed_data = prepare_model_data_extended(data)\n",
        "\n",
        "        print(\"Training model...\")\n",
        "        model_results = train_model_extended(\n",
        "            processed_data,\n",
        "            model_type='ridge',\n",
        "            test_size=26,\n",
        "            feature_selection='transformed',\n",
        "            alpha=100.0\n",
        "        )\n",
        "\n",
        "        print(f\"Model Training Results:\")\n",
        "        print(f\"Training R²: {model_results['train_r2']:.4f}\")\n",
        "        print(f\"Test R²: {model_results['test_r2']:.4f}\")\n",
        "        print(f\"Training RMSE: {model_results['train_rmse']:.2f}\")\n",
        "        print(f\"Test RMSE: {model_results['test_rmse']:.2f}\")\n",
        "\n",
        "        # Calculate ROI\n",
        "        print(\"Calculating ROI...\")\n",
        "        roi_results = calculate_roi_extended(model_results, processed_data)\n",
        "        for channel, roi in roi_results.items():\n",
        "            print(f\"{channel} ROI: {roi:.2f}\")\n",
        "\n",
        "        # Decompose sales\n",
        "        print(\"Decomposing sales...\")\n",
        "        contrib_dict = decompose_sales_extended(model_results, processed_data)\n",
        "\n",
        "        # Simulate budget allocation\n",
        "        print(\"Simulating budget allocations...\")\n",
        "        total_budget = (data['TV_Spend'].sum() + data['Digital_Spend'].sum() +\n",
        "                        data['Radio_Spend'].sum() + data['Print_Spend'].sum() +\n",
        "                        data['Social_Media_Spend'].sum() + data['Influencer_Spend'].sum())\n",
        "\n",
        "        simulation_results = simulate_budget_allocation_extended(\n",
        "            model_results,\n",
        "            processed_data,\n",
        "            budget_total=total_budget,\n",
        "            n_simulations=100,\n",
        "            constraints={\n",
        "                'min_tv': 0.1,\n",
        "                'max_tv': 0.5,\n",
        "                'min_digital': 0.15,\n",
        "                'max_digital': 0.6,\n",
        "                'min_social_media': 0.1,\n",
        "                'max_social_media': 0.4,\n",
        "                'min_influencer': 0.05,\n",
        "                'max_influencer': 0.3\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\"Top budget allocation strategy:\")\n",
        "        best_result = simulation_results[0]\n",
        "        for channel, allocation in best_result['allocation_percentages'].items():\n",
        "            print(f\"{channel}: {allocation:.1f}%\")\n",
        "        print(f\"Predicted Sales: {best_result['total_sales']:.2f}\")\n",
        "\n",
        "        if visualize:\n",
        "            print(\"Creating visualizations...\")\n",
        "            # Create figures\n",
        "            spend_fig = plot_media_spend_patterns_extended(data)\n",
        "            roi_fig = plot_roi_comparison_extended(roi_results)\n",
        "            response_curves_fig = plot_channel_response_curves_extended(processed_data, model_results)\n",
        "            decomp_fig = plot_sales_decomposition_extended(contrib_dict, processed_data)\n",
        "            budget_fig = plot_budget_optimization_results(simulation_results)\n",
        "\n",
        "            # Show figures\n",
        "            print(\"Visualization complete. Figures available in variables:\")\n",
        "            print(\"- spend_fig: Media spend patterns\")\n",
        "            print(\"- roi_fig: ROI comparison\")\n",
        "            print(\"- response_curves_fig: Channel response curves\")\n",
        "            print(\"- decomp_fig: Sales decomposition\")\n",
        "            print(\"- budget_fig: Budget optimization\")\n",
        "\n",
        "            # Return everything\n",
        "            return {\n",
        "                'data': data,\n",
        "                'processed_data': processed_data,\n",
        "                'model_results': model_results,\n",
        "                'roi_results': roi_results,\n",
        "                'contrib_dict': contrib_dict,\n",
        "                'simulation_results': simulation_results,\n",
        "                'figures': {\n",
        "                    'spend_fig': spend_fig,\n",
        "                    'roi_fig': roi_fig,\n",
        "                    'response_curves_fig': response_curves_fig,\n",
        "                    'decomp_fig': decomp_fig,\n",
        "                    'budget_fig': budget_fig\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Return results without figures\n",
        "        return {\n",
        "            'data': data,\n",
        "            'processed_data': processed_data,\n",
        "            'model_results': model_results,\n",
        "            'roi_results': roi_results,\n",
        "            'contrib_dict': contrib_dict,\n",
        "            'simulation_results': simulation_results\n",
        "        }\n",
        "\n",
        "    # Just return the data if not training model\n",
        "    return {'data': data}\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete analysis\n",
        "    results = run_mmm_analysis(periods=104, train_model=True, visualize=True)\n",
        "\n",
        "    # To display a particular visualization (in a Jupyter notebook)\n",
        "    results['figures']['roi_fig'].show()\n",
        "\n",
        "    # To save figures to files\n",
        "    for name, fig in results['figures'].items():fig.write_html(f\"{name}.html\")\n",
        "\n",
        "    # Extract insights from the results\n",
        "    data = results['data']\n",
        "    model_results = results['model_results']\n",
        "    roi_results = results['roi_results']\n",
        "    simulation_results = results['simulation_results']\n",
        "\n",
        "    # Print summary report\n",
        "    print(\"\\n===== MARKETING MIX MODEL SUMMARY REPORT =====\")\n",
        "    print(\"\\nMODEL PERFORMANCE\")\n",
        "    print(f\"Training R²: {model_results['train_r2']:.4f}\")\n",
        "    print(f\"Test R²: {model_results['test_r2']:.4f}\")\n",
        "\n",
        "    print(\"\\nCHANNEL ROI\")\n",
        "    channels_by_roi = sorted(roi_results.items(), key=lambda x: x[1], reverse=True)\n",
        "    for channel, roi in channels_by_roi:\n",
        "        print(f\"{channel}: {roi:.2f}\")\n",
        "\n",
        "    print(\"\\nBUDGET OPTIMIZATION\")\n",
        "    best_allocation = simulation_results[0]['allocation_percentages']\n",
        "    current_allocation = {\n",
        "        'TV': data['TV_Spend'].sum(),\n",
        "        'Digital': data['Digital_Spend'].sum(),\n",
        "        'Radio': data['Radio_Spend'].sum(),\n",
        "        'Print': data['Print_Spend'].sum(),\n",
        "        'Social_Media': data['Social_Media_Spend'].sum(),\n",
        "        'Influencer': data['Influencer_Spend'].sum()\n",
        "    }\n",
        "    total_spend = sum(current_allocation.values())\n",
        "    current_allocation = {k: v/total_spend*100 for k, v in current_allocation.items()}\n",
        "\n",
        "    print(\"Current allocation vs. Optimized allocation:\")\n",
        "    for channel in sorted(best_allocation.keys()):\n",
        "        print(f\"{channel}: {current_allocation[channel]:.1f}% -> {best_allocation[channel]:.1f}% \" +\n",
        "              f\"({'↑' if best_allocation[channel] > current_allocation[channel] else '↓'})\")\n",
        "\n",
        "    print(\"\\nKEY INSIGHTS:\")\n",
        "    # Identify highest ROI channel\n",
        "    best_channel = max(roi_results.items(), key=lambda x: x[1])[0]\n",
        "    print(f\"1. {best_channel} has the highest ROI at {roi_results[best_channel]:.2f}\")\n",
        "\n",
        "    # Compare digital and traditional channels\n",
        "    digital_roi = roi_results['Digital']\n",
        "    social_roi = roi_results['Social_Media']\n",
        "    influencer_roi = roi_results['Influencer']\n",
        "    tv_roi = roi_results['TV']\n",
        "    print(f\"2. Digital channels (Digital: {digital_roi:.2f}, Social: {social_roi:.2f}, \" +\n",
        "          f\"Influencer: {influencer_roi:.2f}) vs. TV: {tv_roi:.2f}\")\n",
        "\n",
        "    # Suggest channel shifts based on optimal allocation\n",
        "    increases = []\n",
        "    decreases = []\n",
        "    for channel in best_allocation:\n",
        "        diff = best_allocation[channel] - current_allocation[channel]\n",
        "        if diff > 5:\n",
        "            increases.append(f\"{channel} (+{diff:.1f}%)\")\n",
        "        elif diff < -5:\n",
        "            decreases.append(f\"{channel} ({diff:.1f}%)\")\n",
        "\n",
        "    if increases:\n",
        "        print(f\"3. Recommended budget increases: {', '.join(increases)}\")\n",
        "    if decreases:\n",
        "        print(f\"4. Recommended budget decreases: {', '.join(decreases)}\")\n",
        "\n",
        "\n",
        " # Modify the prepare_model_data_extended function to handle potential NaN values\n",
        "# Add this code to your script to ensure figures are saved to files\n",
        "\n",
        "def save_mmm_visualizations(results, output_dir=\"mmm_visualizations\"):\n",
        "    \"\"\"\n",
        "    Save all MMM visualizations to HTML files in the specified directory\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : dict\n",
        "        The results dictionary returned by run_mmm_analysis\n",
        "    output_dir : str\n",
        "        Directory to save the visualizations (will be created if it doesn't exist)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Check if figures are in the results\n",
        "    if 'figures' not in results:\n",
        "        print(\"No figures found in results dictionary.\")\n",
        "        return\n",
        "\n",
        "    # Save each figure to a file\n",
        "    for name, fig in results['figures'].items():\n",
        "        file_path = os.path.join(output_dir, f\"{name}.html\")\n",
        "        try:\n",
        "            fig.write_html(file_path)\n",
        "            print(f\"Saved {name} to {file_path}\")\n",
        "\n",
        "            # For Jupyter notebooks, display a link to the file\n",
        "            try:\n",
        "                display(HTML(f'<a href=\"{file_path}\" target=\"_blank\">View {name}</a>'))\n",
        "            except:\n",
        "                pass\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {name}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\nAll figures saved to {output_dir} directory.\")\n",
        "    return\n",
        "\n",
        "# If you're having issues with figures not being generated, try this function to\n",
        "# manually create and save the key visualizations\n",
        "def create_and_save_visualizations(data, model_results, roi_results, simulation_results, contrib_dict=None):\n",
        "    \"\"\"\n",
        "    Manually create and save visualizations from MMM results\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas DataFrame\n",
        "        The original or processed data\n",
        "    model_results : dict\n",
        "        Model results dictionary from train_model_extended\n",
        "    roi_results : dict\n",
        "        ROI results dictionary from calculate_roi_extended\n",
        "    simulation_results : list\n",
        "        Budget simulation results from simulate_budget_allocation_extended\n",
        "    contrib_dict : dict, optional\n",
        "        Sales decomposition dictionary from decompose_sales_extended\n",
        "    \"\"\"\n",
        "    import os\n",
        "    output_dir = \"mmm_visualizations\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Media spend patterns\n",
        "    print(\"Creating media spend patterns visualization...\")\n",
        "    try:\n",
        "        spend_fig = plot_media_spend_patterns_extended(data)\n",
        "        spend_fig.write_html(os.path.join(output_dir, \"spend_fig.html\"))\n",
        "        print(\"Saved media spend patterns visualization.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating media spend patterns visualization: {str(e)}\")\n",
        "\n",
        "    # 2. ROI comparison\n",
        "    print(\"Creating ROI comparison visualization...\")\n",
        "    try:\n",
        "        roi_fig = plot_roi_comparison_extended(roi_results)\n",
        "        roi_fig.write_html(os.path.join(output_dir, \"roi_fig.html\"))\n",
        "        print(\"Saved ROI comparison visualization.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating ROI comparison visualization: {str(e)}\")\n",
        "\n",
        "    # 3. Channel response curves - this may be slow\n",
        "    print(\"Creating channel response curves visualization (this may take a moment)...\")\n",
        "    try:\n",
        "        response_curves_fig = plot_channel_response_curves_extended(data, model_results)\n",
        "        response_curves_fig.write_html(os.path.join(output_dir, \"response_curves_fig.html\"))\n",
        "        print(\"Saved channel response curves visualization.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating channel response curves visualization: {str(e)}\")\n",
        "\n",
        "    # 4. Sales decomposition - only if contrib_dict is provided\n",
        "    if contrib_dict is not None:\n",
        "        print(\"Creating sales decomposition visualization...\")\n",
        "        try:\n",
        "            decomp_fig = plot_sales_decomposition_extended(contrib_dict, data)\n",
        "            decomp_fig.write_html(os.path.join(output_dir, \"decomp_fig.html\"))\n",
        "            print(\"Saved sales decomposition visualization.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating sales decomposition visualization: {str(e)}\")\n",
        "\n",
        "    # 5. Budget optimization\n",
        "    print(\"Creating budget optimization visualization...\")\n",
        "    try:\n",
        "        budget_fig = plot_budget_optimization_results(simulation_results)\n",
        "        budget_fig.write_html(os.path.join(output_dir, \"budget_fig.html\"))\n",
        "        print(\"Saved budget optimization visualization.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating budget optimization visualization: {str(e)}\")\n",
        "\n",
        "    print(f\"\\nAll available visualizations have been saved to {output_dir} directory.\")\n",
        "\n",
        "# Example usage (add this to your code):\n",
        "\"\"\"\n",
        "# After running your analysis\n",
        "results = run_mmm_analysis(periods=104, train_model=True, visualize=True)\n",
        "\n",
        "# Save all visualizations\n",
        "save_mmm_visualizations(results)\n",
        "\n",
        "# Alternatively, create and save visualizations manually\n",
        "create_and_save_visualizations(\n",
        "    results['data'],\n",
        "    results['model_results'],\n",
        "    results['roi_results'],\n",
        "    results['simulation_results'],\n",
        "    results.get('contrib_dict')\n",
        ")\n",
        "\"\"\"\n",
        "print(\"\\n===== END OF REPORT =====\")"
      ]
    }
  ]
}